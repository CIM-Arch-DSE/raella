{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **RAELLA Model**\n",
    "## **Overview**\n",
    "This is the Accelergy/Timeloop model of the RAELLA Processing-In-Memory (PIM) Deep Neural Network (DNN) accelerator architecture from \"RAELLA: Reforming the Arithmetic for Efficient, Low-Resolution, and Low-Loss Analog PIM: No Retraining Required!\" by Tanner Andrulis, Vivienne Sze, and Joel Emer (https://dl.acm.org/doi/10.1145/3579371.3589062).\n",
    "\n",
    "The models includes several parameterized RAELLA architectures, plus a model of the ISAAC architecture from \"ISAAC: A Convolutional Neural Network Accelerator with In-Situ Analog Arithmetic in Crossbars\" by Ali Shafiee et. al. (https://dl.acm.org/doi/10.1145/3007787.3001139).\n",
    "\n",
    "This model provides estimates of area, energy, and throughput for the architectures while they run DNN layers. All DNNs used in the RAELLA paper are included.\n",
    "\n",
    "## **Table of Contents**\n",
    "1. [Getting Started](#Getting-Started)\n",
    "2. [Speeding Up Mapping](#speeding-up-mapping)\n",
    "3. [Collecting Input Files And Putting Together The Specification](#collecting-input-files-and-putting-together-the-specification)\n",
    "3. [Mapping a DNN Layer to the Architecture](#mapping-a-dnn-layer-to-the-architecture)\n",
    "4. [Plotting Energy, Area, and Throughput Results](#plotting-energy-area-and-throughput-results)\n",
    "5. [One-Layer Ablation Experiment](#one-layer-ablation-experiment)\n",
    "6. [Multi-Layer Ablation Experiment](#multi-layer-ablation-experiment)\n",
    "7. [Full-DNN Ablation Experiment](#full-dnn-ablation-experiment)\n",
    "8. [Full-DNN Mapping with Energy, Area, And Throughput Results](#full-dnn-mapping-with-energy-area-and-throughput-results)\n",
    "9. [Tips for Extending the Model](#tips-for-extending-the-model)\n",
    "10. [Contact](#contact)\n",
    "11. [Glossary](#glossary)\n",
    "12. [References](#references)\n",
    "\n",
    "\n",
    "## **Getting Started**\n",
    "This Jupyter notebook guides you through running the model on any of the provided architectures or DNNs. It starts off with a single-DNN-layer, single-architecture example, and then shows how to run multiple DNN layers and multiple architectures, building up to a full ablation study over a single DNN.\n",
    "\n",
    "Once you're familiar with the model, you can use/edit any of the provided scripts or YAML files to build your own experiments.\n",
    "\n",
    "We'll start off by importing the necessary Python packages and setting defining paths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import timeloopfe as tl\n",
    "from timeloopfe.v4spec import Specification\n",
    "from scripts.processors import ArtifactProcessor, GreedyMapperProcessor\n",
    "from timeloopfe.processors.v4_standard_suite import STANDARD_SUITE\n",
    "from subprocess import PIPE\n",
    "from scripts.helper_functions import *\n",
    "import tqdm\n",
    "from typing import Optional\n",
    "\n",
    "\n",
    "thisdir = os.getcwd()\n",
    "\n",
    "# Stores recorded values from PyTorch kernels to be used in the simulator.\n",
    "# Values include input/weight value statistics, speculation success rates, and\n",
    "# choices for RAELLA's Adaptive Weight Slicing.\n",
    "STATS_PATH = f\"{thisdir}/recorded_statistics\"\n",
    "\n",
    "# Architecture models of RAELLA and ISAAC.\n",
    "ARCHITECTURE_PATH = f\"{thisdir}/models\"\n",
    "\n",
    "# Path to the mapper configuration file. \n",
    "MAPPER_PATH = f\"{thisdir}/mapper.yaml\"\n",
    "\n",
    "# Environment variables to be used in Timeloop. We want scientific notation\n",
    "# output.\n",
    "env = {\n",
    "    \"TIMELOOP_OUTPUT_STAT_SCIENTIFIC\": 1, # Outputs scientific notation\n",
    "    \"TIMELOOP_OUTPUT_STAT_DEFAULT_FLOAT\": 0, # Outputs default floating-point\n",
    "}\n",
    "\n",
    "# Where Timeloop/Accelergy/TimeloopFE will do work.\n",
    "RUN_DIRECTORY = f\"{thisdir}/rundir\"\n",
    "OUTPUT_STATS_PATH = f\"{RUN_DIRECTORY}/timeloop_mapper.stats.txt\"\n",
    "OUTPUT_ART_PATH = f\"{RUN_DIRECTORY}/timeloop_mapper.ART.yaml\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Speeding Up Mapping** <a name=\"speeding-up-mapping\"></a>\n",
    "To speed up mapping, we use a timeloopfe processor to generate constraints on\n",
    "our mapspace. Constraints are dependent on the architecture and the DNN layer,\n",
    "so the processor is run for each combination.\n",
    "\n",
    "The processor reduces the size of the mapspace by many orders of magnitude,\n",
    "allowing us to run experiments in a reasonable amount of time. Of course, any\n",
    "mappings we find here are a subset of what Timeloop searches, but they are\n",
    "reasonable constraints to make. The following are the constraints we use and\n",
    "why they are reasonable:\n",
    "\n",
    "- Pack the weights as densely as possible into the crossbars. This is\n",
    "  reasonable because better utilization of the crossbars is always better for\n",
    "  ISAAC/RAELLA, and all buffers are provisioned to handle maximally-utilized\n",
    "  crossbars, so we won't get buffer overflows from doing this.\n",
    "- Replicate weights over N/Q (output batches / output columns) across tiles\n",
    "  after all weights are mapped. This is reasonable because, after confirming we\n",
    "  have extra space, working with more tiles in parallel is helpful. We have\n",
    "  different tiles work over the dimensions with the LEAST reuse so that each\n",
    "  individual tile has more reuse. This may not return the\n",
    "  data-movement-energy-optimal result, but it's close enough that the\n",
    "  differences don't matter much.\n",
    "- Restrict temporal permutations such that P (horizontal traversal of outputs),\n",
    "  then Q (vertical traversal of outputs), then N (different samples) is the\n",
    "  order for all buffers. This is reasonable because RAELLA tiles consume input\n",
    "  rows one at a time and produce output rows one at a time. Generally, moving\n",
    "  within a row gets more reuse than moving between rows, which gets more reuse\n",
    "  than moving between samples. This may not be true always, but data movement\n",
    "  energy is not the dominant factor in RAELLA/ISAAC, so we don't worry about\n",
    "  it too much."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: A description of all the dimensions (CRSLMPGNQ) and architecture levels\n",
    "# (tile, macro, crossbar, column, cim_unit) can be found in the end of this\n",
    "# notebook. \n",
    "greedy_mapper_processors = [\n",
    "    # 1. PACK WEIGHTS AS DENSELY AS POSSIBLE.    \n",
    "    # First map input channels over cim_units. Spill if needed.\n",
    "    GreedyMapperProcessor(\"CRS\", (\"cim_unit\"), no_other_factors=True),\n",
    "    GreedyMapperProcessor(\"CRS\", (\"crossbar\", \"macro\", \"tile\")),\n",
    "    # Allocate weight slices to columns.\n",
    "    GreedyMapperProcessor(\"L\", (\"column\")),\n",
    "    # Allocate output channels to columns. Spill if necessary.\n",
    "    GreedyMapperProcessor(\"M\", (\"column\", \"crossbar\", \"macro\", \"tile\")),\n",
    "    # Map G at the highest levels possible. There is no reuse across the G\n",
    "    # dimension, so keeping it local isn't helpful.\n",
    "    GreedyMapperProcessor(\"G\", (\"tile\", \"macro\", \"crossbar\")),\n",
    "    \n",
    "    # 2. REPLICATE WEIGHTS WITH ANY EXTRA SPACE.\n",
    "    # We've mapped all the weights already, so the remaining space at the tile\n",
    "    # level can go to N/Q\n",
    "    GreedyMapperProcessor(\"NQ\", (\"tile\")),\n",
    "    \n",
    "    # 3. APPLY INTELLIGENT RESTRICTIONS TO TEMPORAL REUSE.\n",
    "    # Reuse across P (horizontal traversal of outputs, within-row) is\n",
    "    # most helpful, so do that innermost. Reuse across Q (vertical traversal of\n",
    "    # outputs, within-column) is less helpful, so do that next. Reuse across N\n",
    "    # (different samples within batch) is least helpful, so do that outermost.\n",
    "    GreedyMapperProcessor(temporal_permutations=\"NQP\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Collecting Input Files And Putting Together The Specification**\n",
    "The following function creates a timeloopfe specification for a given\n",
    "parameterization of RAELLA/ISAAC running a DNN layer. Comments explain each\n",
    "piece."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_spec(\n",
    "        architecture: str, \n",
    "        dnn: str, \n",
    "        layer: str, \n",
    "        use_typical_statistics: bool,\n",
    "        n_tiles: Optional[int] = None,\n",
    "        batch_size: Optional[int] = None,\n",
    "    ) -> Specification:\n",
    "    architecture = architecture.lower()\n",
    "    dnn = dnn.lower()\n",
    "    layer = layer.lower()\n",
    "\n",
    "    # The recorded statistics are stored in two places: typical_values and\n",
    "    # torch_values. Torch values are recorded from PyTorch kernels for specific\n",
    "    # layers. Typical values are general-case, recommended for use with unseen\n",
    "    # layers.\n",
    "    if use_typical_statistics:\n",
    "        stats_path = f\"{STATS_PATH}/typical_values/{architecture}\"\n",
    "    else:\n",
    "        stats_path = f\"{STATS_PATH}/torch_values/{architecture}/{dnn}/\"\n",
    "    \n",
    "    # The ArtifactProcessor does a few things:\n",
    "    # 1. Reads & populates the recorded statistics.\n",
    "    # 2. For ISAAC designs, reads some additional values from the layer YAML.\n",
    "    #    The values read are the per-bit probability of a weight/input having a\n",
    "    #    value of 1. These values are already handled in the RAELLA recorded\n",
    "    #    statistics.\n",
    "    # 3. For QDQBERT, RAELLA must process positive/negative inputs in two\n",
    "    #    separate passes. This processor adds the appropriate adjustments. \n",
    "    artifact_processor = ArtifactProcessor(\n",
    "        isaac=\"isaac\" in architecture,\n",
    "        raella_bert=\"raella\" in architecture and \"qdqbert\" in dnn.lower(),\n",
    "        vars_from=stats_path,\n",
    "        use_typical_statistics=use_typical_statistics,\n",
    "    )\n",
    "    \n",
    "    # The other RAELLA designs are just parameterized versions of the RAELLA\n",
    "    # architecture, and use the same architecture specification.\n",
    "    yamldir = f'{ARCHITECTURE_PATH}/{architecture}/'\n",
    "    if 'raella' in architecture:\n",
    "        yamldir = f'{ARCHITECTURE_PATH}/raella/'\n",
    "    \n",
    "    # Top-level architecture YAML file.\n",
    "    arch_path = f'{yamldir}/arch.yaml'\n",
    "    # Variables with which to populate the architecture.\n",
    "    variables_path = f'{yamldir}/variables.yaml'\n",
    "    # Components that are common to all architectures.\n",
    "    common_components_path = f'{ARCHITECTURE_PATH}/common_components/*.yaml'\n",
    "    # Layer YAML file.\n",
    "    layer_path = f'{thisdir}/dnn_layers/{dnn}/{layer}.yaml'\n",
    "    # Mapper YAML file.\n",
    "    mapper_path = f'{thisdir}/mapper.yaml'\n",
    "    # Put togther processors. The STANDARD_SUITE is a suite of processors\n",
    "    # that are generally recommended. The order is important, as:\n",
    "    # 1. The artifact processor populates variables\n",
    "    # 2. The math processor in the standard suite uses those variables\n",
    "    # 3. The constraint attacher processor in the standard suite organizes\n",
    "    #    constraints in the YAML files.\n",
    "    # 4. The greedy mapper processors use those constraints.\n",
    "    procs = [artifact_processor] + STANDARD_SUITE + greedy_mapper_processors\n",
    "    \n",
    "    spec = Specification.from_yaml_files(\n",
    "        arch_path,\n",
    "        variables_path,\n",
    "        common_components_path,\n",
    "        mapper_path,\n",
    "        layer_path,\n",
    "        processors=procs\n",
    "    )\n",
    "    \n",
    "    # Set variables\n",
    "    if batch_size is not None:\n",
    "        spec.variables['BATCH_SIZE'] = batch_size\n",
    "    if n_tiles is not None:\n",
    "        spec.variables['TILES_PER_ARCH'] = n_tiles\n",
    "    \n",
    "    spec.process() # Run the processors.\n",
    "    return spec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Mapping a DNN Layer to the Architecture**\n",
    "We can now run the mapper. There are several DNNs to choose from, and several\n",
    "parameterizations of RAELLA/ISAAC. Set the DNN_CHOICE, ARCHITECTURE_CHOICE,\n",
    "LAYER_CHOICE, USE_TYPICAL_STATISTICS, N_TILES, and BATCH_size parameters below\n",
    "to choose which setup to run.\n",
    "\n",
    "The tl.mapper command will run Timeloop mapper to find the best mapping for the\n",
    "given architecture and DNN layer. It should take from 6-60 seconds to run\n",
    "depending on the settings and machine.\n",
    "\n",
    "The four architectures included are the architectures used in the ablation\n",
    "study of RAELLA. They are:\n",
    "- isaac: An 8b ISAAC. Crossbars are 128 by 128 1T1R devices running unsigned\n",
    "  arithmetic. Eight bit weights are sliced into four slices of two bits each.\n",
    "  Eight bit inputs are sliced into eight slices of one bit each. ADCs are 8b.\n",
    "\n",
    "- raella_no_speculation_2b_cells: This is RAELLA running Center+Offset, its\n",
    "  first contribution. Crossbars are 512 by 512 2T2R devices running Center+Offset\n",
    "  arithmetic. Eight bit weights are sliced into four slices of two bits each.\n",
    "  Eight bit inputs are sliced into eight slices of one bit each. ADCs are 7b.\n",
    "\n",
    "- raella_no_speculation: This is RAELLA running Center+Offset and Adaptive\n",
    "  Weight Slicing, its first two contributions. The same setup is used as\n",
    "  raella_no_speculation_2b_cells, but the weight slicings are chosen per-layer.\n",
    "  Most layers use three slices in a 4b-2b-2b pattern.\n",
    "\n",
    "- raella: This is RAELLA running Center+Offset, Adaptive Weight Slicing, and\n",
    "  Speculation; all three contributions. The same setup is used as\n",
    "  raella_no_speculation, but speculation is enabled. RAELLA runs a 2-4 bit\n",
    "  speculation input slice followed by 2-4 one-bit recovery input slices. In\n",
    "  recovery cycles, ADCs do not convert columns where speculation succeeded.\n",
    "  Overall, this reduces ADC usage by approximately 60%, at the cost of lower\n",
    "  throughput and increased energy for other components.\n",
    "\n",
    "You can expect slightly different energy results from the ablation study in the\n",
    "RAELLA paper. This is because:\n",
    "- We're finding different mappings here. The original paper didn't use the\n",
    "  greedy mapper-- instead, Timeloop was run for multiple days for each DNN.\n",
    "- Models of networks in Timeloop have been improved. \n",
    "- There was a mistake in the output register model in the original paper,\n",
    "  resulting in doubled output register size for the RAELLA models. This has\n",
    "  been fixed, which reduces output register area / energy for RAELLA\n",
    "  parameterizations.\n",
    "- There was a mistake in ISAAC's memory cell energy in the original paper,\n",
    "  where input bits were counted with a different encoding. This has been fixed,\n",
    "  which reduces memory cell (crossbar) energy for ISAAC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DNNS = [\n",
    "    \"InceptionV3\",   # Layers 000-094 [3]\n",
    "    \"GoogLeNet\",     # Layers 000-057 [4]\n",
    "    \"ResNet18\",      # Layers 000-020 [5]\n",
    "    \"ResNet50\",      # Layers 000-053 [5]\n",
    "    \"MobileNetV2\",   # Layers 000-052 [6]\n",
    "    \"shufflenet1_0\", # Layers 000-056 [7]\n",
    "    \"qdqbert\",       # Layers 000-071 [8][9]\n",
    "]\n",
    "\n",
    "N_LAYERS = {k: len(os.listdir(f\"{thisdir}/dnn_layers/{k.lower()}\")) for k in DNNS}\n",
    "\n",
    "ARCHITECTURES = [\n",
    "    \"isaac_like\",                      # 1st ablation in paper\n",
    "    \"raella_no_speculation_2b_cells\",  # 2nd ablation in paper\n",
    "    \"raella_no_speculation\",           # 3rd ablation in paper\n",
    "    \"raella\",                          # RAELLA, 4th ablation in paper\n",
    "]\n",
    "\n",
    "# SET THE VALUES BELOW. SET THE VALUES BELOW. SET THE VALUES BELOW.\n",
    "# SET THE VALUES BELOW. SET THE VALUES BELOW. SET THE VALUES BELOW.\n",
    "# SET THE VALUES BELOW. SET THE VALUES BELOW. SET THE VALUES BELOW.\n",
    "DNN_CHOICE = 'ResNet18' # Which DNN to use.\n",
    "ARCHITECTURE_CHOICE = 'raella' # Which architecture to use.\n",
    "LAYER_CHOICE = 10 # Which layer to use.\n",
    "USE_TYPICAL_STATISTICS = False # Use typical statistics for the layer\n",
    "                               # instead of recorded statistics.\n",
    "N_TILES = 16\n",
    "BATCH_SIZE = 32 # Set this too low and utilization may drop for the number of\n",
    "                # tiles available.\n",
    "# SET THE VALUES ABOVE. SET THE VALUES ABOVE. SET THE VALUES ABOVE.\n",
    "# SET THE VALUES ABOVE. SET THE VALUES ABOVE. SET THE VALUES ABOVE.\n",
    "# SET THE VALUES ABOVE. SET THE VALUES ABOVE. SET THE VALUES ABOVE.\n",
    "\n",
    "assert DNN_CHOICE in DNNS\n",
    "assert LAYER_CHOICE < N_LAYERS[DNN_CHOICE]\n",
    "assert ARCHITECTURE_CHOICE in ARCHITECTURES\n",
    "\n",
    "layer = f\"{LAYER_CHOICE:03d}\"\n",
    "\n",
    "\n",
    "spec = get_spec(ARCHITECTURE_CHOICE, DNN_CHOICE, layer, \n",
    "                USE_TYPICAL_STATISTICS, N_TILES, BATCH_SIZE)\n",
    "\n",
    "os.system(f\"rm -rf {OUTPUT_STATS_PATH}\")\n",
    "proc = tl.mapper(\n",
    "    spec, \n",
    "    RUN_DIRECTORY, \n",
    "    env, \n",
    "    dump_intermediate_to=f'{RUN_DIRECTORY}/input.yaml', \n",
    "    return_proc=True, \n",
    "    log_to=PIPE\n",
    ")\n",
    "while proc.poll() is None:\n",
    "    print(proc.stdout.readline().decode('utf-8'), end='')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Plotting Energy, Area, and Throughput Results**\n",
    "Now that Timeloop has run, outputs will appear in RUN_DIRECTORY. Of interest\n",
    "are timeloop_mapper.map.txt and timeloop_mapper.stats.txt. The former contains\n",
    "the mapping, and the latter contains the performance and energy results.\n",
    "\n",
    "The get_energy_cycles and get_area_from_art functions parse these files and\n",
    "return the relevant results. Let's plot the energy and area of each component:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats, cycles = get_energy_cycles(OUTPUT_STATS_PATH)\n",
    "for_plot = prep_numbers_for_plot(stats)\n",
    "for_plot = {k: v for k, v in for_plot.items() if v != 0}\n",
    "area_for_plot = prep_numbers_for_plot(get_area_from_art(OUTPUT_ART_PATH))\n",
    "area_for_plot = {k: v for k, v in area_for_plot.items() if v != 0}\n",
    "n_tiles = spec.variables[\"TILES_PER_ARCH\"]\n",
    "\n",
    "plot_results(\n",
    "    {f'{ARCHITECTURE_CHOICE} {DNN_CHOICE} {layer}': for_plot},\n",
    "    {f'{ARCHITECTURE_CHOICE} {DNN_CHOICE} {layer}': area_for_plot},\n",
    "    {f'{ARCHITECTURE_CHOICE} {DNN_CHOICE} {layer}': {'Cycles': cycles}},\n",
    ")\n",
    "\n",
    "print(f'Total cycles: {cycles}. Total energy: {sum(for_plot.values()) / 1e6:.2f} uJ.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **One-Layer Ablation Experiment**\n",
    "We can also run ablations to tell how each of RAELLA's contributions impacts\n",
    "the overall energy of the system. We'll run a one-layer ablation experiment\n",
    "using our four architectures and show the results on one plot.\n",
    "\n",
    "We can see that adding Center+Offset, the second bar, reduces the energy of nearly all components relative to the ISAAC-like design. It enables using larger crossbars/crossbars, which complete more analog operations in parallel. This greater parallelism better amortizes data movement and ADC energy.\n",
    "\n",
    "Adding in Adaptive Weight Slicing, the third bar, is almost always beneficial to energy. It usually uses fewer slices, reducing the number of ADC converts required. However, it is not always beneficial. If it uses many slices, (e.g., the last layer of each DNN), there are energy penalties.\n",
    "\n",
    "Adding in Speculation substantially reduces ADC energy, trading off increased energy in other components as well as reduced throughput. The increased energy and decreased throughput comes from running speculation and recovery cycles, while the reduced ADC energy comes from not converting columns where speculation succeeded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(arch, dnn, layer, desc_text=''):\n",
    "    if isinstance(layer, list):\n",
    "        total = {}\n",
    "        per_layer_cycles = []\n",
    "        for l in tqdm.tqdm(layer, desc=desc_text):\n",
    "            e, a, c = run(arch, dnn, l)\n",
    "            per_layer_cycles.append(c)\n",
    "            for k, v in e.items():\n",
    "                total[k] = total.get(k, 0) + v\n",
    "        return total, a, per_layer_cycles\n",
    "    if isinstance(layer, int):\n",
    "        layer = f\"{layer:03d}\"\n",
    "    \n",
    "    spec = get_spec(arch, dnn, layer, USE_TYPICAL_STATISTICS,  N_TILES, BATCH_SIZE)\n",
    "    os.system(f'rm {OUTPUT_STATS_PATH} {OUTPUT_ART_PATH}')\n",
    "    proc = tl.mapper(\n",
    "        spec,\n",
    "        RUN_DIRECTORY,\n",
    "        env,\n",
    "        dump_intermediate_to=f'{RUN_DIRECTORY}/input.yaml',\n",
    "        return_proc=True,\n",
    "        log_to='/dev/null',\n",
    "    )\n",
    "    while proc.poll() is None:\n",
    "        pass\n",
    "    stats, cycles = get_energy_cycles(OUTPUT_STATS_PATH)\n",
    "    energy_for_plot = prep_numbers_for_plot(stats)\n",
    "    energy_for_plot = {k: v for k, v in energy_for_plot.items() if v != 0}\n",
    "    art = get_area_from_art(OUTPUT_ART_PATH)\n",
    "    area_for_plot = prep_numbers_for_plot(art)\n",
    "    area_for_plot = {k: v for k, v in area_for_plot.items() if v != 0}\n",
    "    cycles_for_plot = {'Cycles': cycles}\n",
    "    return energy_for_plot, area_for_plot, cycles_for_plot\n",
    "\n",
    "def ablation_triple_plot(dnn, layer):\n",
    "    arch2energy = {}\n",
    "    arch2area = {}\n",
    "    arch2cycles = {}\n",
    "    for arch in tqdm.tqdm(ARCHITECTURES, desc=f'Layer {layer} of {dnn}'):\n",
    "        e, a, c = run(arch, dnn, layer)\n",
    "        arch2energy[arch] = e\n",
    "        arch2area[arch] = a\n",
    "        arch2cycles[arch] = c\n",
    "    plot_results(arch2energy, arch2area, arch2cycles)\n",
    "\n",
    "\n",
    "ablation_triple_plot(DNN_CHOICE, LAYER_CHOICE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Multi-Layer Ablation Experiment**\n",
    "The performance of each architecture is impacted by the particular DNN layer.\n",
    "The shape (e.g. number of input/output channels, batch size, etc.) of the layer\n",
    "affects the utilization of the crossbar crossbars, the reuse opportunities in the\n",
    "input and output buffers, and the number of cycles required to process the\n",
    "layer. \n",
    "\n",
    "To get a better sense of the performance of each architecture, we can run a\n",
    "similar ablation experiment over more layers. We'll run the first, middle, and\n",
    "last layers of our DNN and plot the results.\n",
    "\n",
    "We can see that the trends seen from the one-layer ablation experiment hold for\n",
    "most layers, but there are exceptions. Note the third set of plots, which runs\n",
    "the last layer of each DNN. In this case, Adaptive Weight Slicing is not\n",
    "beneficial for energy. This is because the last layer is generally much less\n",
    "energy intensive than earlier layers, but it is also very important for high\n",
    "accuracy. Adaptive Weight Slicing uses as many slices as possible in this\n",
    "layer, as this layer has an outsized effect on DNN accuracy relative to its\n",
    "energy cost. This is a worthwhile tradeoff; the last layer is generally much\n",
    "less energy intensive than earlier layers, but it is also very important for\n",
    "high accuracy. *Note: You will not see this effect if USE_TYPICAL_STATISTICS is\n",
    "set to True. The last layer is atypical, which makes it extra interesting.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_triple_plot(DNN_CHOICE, 0)\n",
    "ablation_triple_plot(DNN_CHOICE, N_LAYERS[DNN_CHOICE] // 2)\n",
    "ablation_triple_plot(DNN_CHOICE, N_LAYERS[DNN_CHOICE] - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Full-DNN Ablation Experiment**\n",
    "Now we're ready to see the results for a full DNN. We'll run the same four\n",
    "setups, this time summing the energy across all layers of the DNN.\n",
    "\n",
    "Go grab a coffee. This one will take a while. It should take anywhere from 20\n",
    "minutes to a few hours depending on the machine and the chosen DNN. ResNet18\n",
    "is the fastest.\n",
    "\n",
    "We first run all layers and collect results for the energy, area, and\n",
    "throughput of each layer. Using the throughput information, we will perform\n",
    "replication of layers to maximize throughput. We will then plot the energy,\n",
    "area, and throughput of each architecture for the full DNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_full_dnn(arch, dnn, desc_text=''):\n",
    "    layers = list(range(N_LAYERS[dnn]))\n",
    "    results = run(arch, dnn, layers,  desc_text=desc_text)\n",
    "    return results\n",
    "\n",
    "def run_full_dnn_all_architectures(dnn):\n",
    "    e_results = {}\n",
    "    a_results = {}\n",
    "    c_results = {}\n",
    "    for i, arch in enumerate(ARCHITECTURES):\n",
    "        t = f'Architecture {i+1}/{len(ARCHITECTURES)}'\n",
    "        e, a, c = run_full_dnn(arch, dnn, desc_text=t)\n",
    "        e_results[arch] = e\n",
    "        a_results[arch] = a\n",
    "        c_results[arch] = c\n",
    "    return e_results, a_results, c_results\n",
    "\n",
    "e_results, a_results, c_results = run_full_dnn_all_architectures(DNN_CHOICE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Full-DNN Mapping with Energy, Area, And Throughput Results**\n",
    "We can now plot the results. We'll plot the energy, area, and throughput of\n",
    "each architecture for the full DNN. All tiles will run in a pipelined fashion,\n",
    "sending data from one tile to the next as soon as it is ready.\n",
    "\n",
    "We can increase throughput by replicating layers. If a layer has low\n",
    "throughput, it is replicated, or its weights are copied, to multiple tiles.\n",
    "These tiles run in parallel, increasing throughput.\n",
    "\n",
    "Below, we first adjust the number of tiles of each architecture to normalize\n",
    "area and provide a fair comparison. Then, we perform replication of layer\n",
    "weights to maximize throughput for each architecture. Replication follows a\n",
    "greedy scheme; while there are tiles left, the tiles allocated to the\n",
    "lowest-throughput layer are doubled.\n",
    "\n",
    "Throughput values may differ from the RAELLA paper. The reasons are:\n",
    "- Throughput is dependent of number of tiles, which is different here than in\n",
    "  the paper.\n",
    "- All mappings in the paper were manually checked to ensure good throughput for\n",
    "  each layer. Here, we use a greedy mapper and quick mapping, so this may or\n",
    "  may not be the case.\n",
    "- In the paper, the mapper run for a very long time for each DNN/architecture\n",
    "  combination. Here, each DNN/architecture combination is run for a few\n",
    "  thread-hours at most. This is enough to get a good mapping, but not\n",
    "  necessarily the best mapping. For the paper, we ran for several thread-weeks\n",
    "  per DNN/architecture combination.\n",
    "- The paper uses a different replication scheme. Here, the lowest-throughput\n",
    "  layer tiles are doubled. In the paper, the tiles are incremented. This is\n",
    "  slower to compute, but resulted in better throughput."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The granularity of our throughput estimation is limited by N_TILES, as no\n",
    "# layer can receive fewer than N_TILES tiles. However, we can't make N_TILES\n",
    "# too small or some layers may not be able to be mapped. In the paper, we solve\n",
    "# this by calculating the minimal tiles for each layer individually and use\n",
    "# that as a start point. Here, we can do a few things:\n",
    "# 1. Use a big N_TILES to ensure that all layers can be mapped.\n",
    "# 2. Use a big batch size such that when layers don't fill up the larger\n",
    "#    N_TILES, they can be automatically replicated to use the extra tiles.\n",
    "# 3. Use a really big AGGREGATE_TILES so that effects of the large N_TILES and\n",
    "#    coarse-grained pipeline does not affect throughput too much.\n",
    "# 4. Pay attention to the normalized throughput, not absolute, because the\n",
    "#    absolute throughput will be very high with this large number of tiles.\n",
    "AGGREGATE_TILES = 1024 * 64\n",
    "\n",
    "def get_throughput_adj_area(e_results, c_results, a_results, AGGREGATE_TILES):\n",
    "    throughput = {}\n",
    "    tiles = {}\n",
    "\n",
    "    # Normalize the number of tiles\n",
    "    first_arch = list(e_results.keys())[0]\n",
    "    area_base = sum(a_results[first_arch].values())\n",
    "\n",
    "    # Replicate layers to maximize throughput\n",
    "    for arch in e_results:\n",
    "        cycles = c_results[arch]\n",
    "        area = a_results[arch]\n",
    "        # Normalize area\n",
    "        n_tiles_total = AGGREGATE_TILES // (sum(area.values()) / area_base)\n",
    "        tiles[arch] = n_tiles_total\n",
    "\n",
    "        # Replicate layers to maximize throughput\n",
    "        layers = list([N_TILES, 1 / c['Cycles'] * 1e9] for c in cycles)\n",
    "        while sum(l[0] for l in layers) < n_tiles_total:\n",
    "            used_tiles = sum(l[0] for l in layers)\n",
    "            slowest_layer = min(layers, key=lambda l: l[1])\n",
    "            if used_tiles + slowest_layer[0] > n_tiles_total:\n",
    "                break\n",
    "            slowest_layer[0] *= 2\n",
    "            slowest_layer[1] *= 2\n",
    "        \n",
    "        slowest_layer = min(layers, key=lambda l: l[1])\n",
    "        throughput[arch] = {'Throughput': slowest_layer[1]}\n",
    "        print(\n",
    "            f'\\t{arch} allocated {sum(l[0] for l in layers)} '\n",
    "            f'tiles with {slowest_layer[1]} batches/s'\n",
    "        )\n",
    "        # for i, l in enumerate(layers):\n",
    "        #     print(f'\\t\\tLayer {i}: {l[0]} tiles, {l[1]:.2f} batches/s')\n",
    "        \n",
    "    area = {a: \n",
    "        {k: v * tiles[a] / N_TILES for k, v in a_results[a].items()} \n",
    "        for a in e_results\n",
    "    }\n",
    "        \n",
    "    return throughput, area\n",
    "\n",
    "throughput, area = get_throughput_adj_area(\n",
    "    e_results, c_results, a_results, AGGREGATE_TILES)\n",
    "print(f'\\nDNN: {DNN_CHOICE}')\n",
    "plot_results(e_results, area, throughput=throughput, energy_mj=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## **Tips for Extending the Model**\n",
    "This concludes the tutorial. The following two sections give some tips and\n",
    "information for running your own experiments with these models.\n",
    "\n",
    "### Interpreting the Stats File\n",
    "When running your own models, you'll likely need to read the Timeloop output\n",
    "stats file (timeloop_mapper.stats.txt). This file contains the energy and\n",
    "latency results for the model.\n",
    "\n",
    "RAELLA and ISAAC both run with 1ns cycles, so the reported number of cycles is\n",
    "equal to the time per layer in nanoseconds. Note that this model does NOT\n",
    "report the latency, only the throughput. Latency would include additional\n",
    "factors such as initial buffer fills and network latency, which is not modeled. \n",
    "\n",
    "The aggregate energy of each component can be found by multiplying its energy\n",
    "per op with the number of computes.\n",
    "\n",
    "Note that the Timeloop reported number of computes is NOT the number of\n",
    "multiply-accumulates. This is because these models record each input, weight,\n",
    "and output bit combination as a different \"op\". Actual multiply-accumulates are\n",
    "equal to: (number of computes) / (number of input bits) / (number of weight\n",
    "bits) / (number of output bits).\n",
    "\n",
    "In addition to the relation between computes and number of bits of each\n",
    "operand, special considerations need to be made for RAELLA. In the simulation\n",
    "for RAELLA's adaptive weight slicing, we change the number of weight bits that\n",
    "the model sees on a per-layer basis (NOTE: Actual number of weight bits is\n",
    "unchanged. This is just how we implemented the model for Timeloop. All layers\n",
    "of all provided models use 8b weights.). For each layer, we need to look at the\n",
    "number of weight bits provided to the model and divide by that value to get the\n",
    "number of multiply-accumulates.\n",
    "\n",
    "### Tips for Running Your Own Experiments\n",
    "Use the following tips to run your own experiments:\n",
    "- To speed experiments, you can parallelize Timeloop calls across DNNs and\n",
    "  layers. If you do this, ensure that each experiment has its own\n",
    "  RUN_DIRECTORY. Additionally, some Accelergy plug-ins may crash when run in\n",
    "  parallel. To avoid this, spawn processes with a few seconds of delay between\n",
    "  them. Also have a re-try that tries a few times if a process crashes.\n",
    "- Use the GreedyMapper wherever possible; in this notebook, it reduces the\n",
    "  mapping search space by eight orders of magnitude or more.\n",
    "- Read & understand the interactions between the architecture, variables, and\n",
    "  macro YAML files.\n",
    "- If Timeloop cannot find a mapping, or it finds mappings that are weird /\n",
    "  obviously suboptimal, then the problem is likely one of the following:\n",
    "  - The mapper timed out before finding a good mapping: To fix this, it is\n",
    "    most effective to constrain the mapping or use the GreedyMapper. Either of\n",
    "    these options can reduce the search space by orders of magnitude.\n",
    "    Additionally, you may increase the timeout and victory condition in the\n",
    "    mapper YAML file to run longer.\n",
    "  - There may be insufficient tiles to map the DNN layer: To fix this,\n",
    "    increase the number of tiles in the architecture YAML file.\n",
    "  - Something is wrong with the architecture: To fix this, enable diagnostics\n",
    "    in the mapping YAML file to see why mapping is failing. I often find myself\n",
    "    following the following procedure:\n",
    "    - Make all the buffers gigantic (depth = big number). Make all the fanouts\n",
    "      gigantic (meshX = big number). The architecture should now work.\n",
    "    - One at a time, return a fanout / buffer to its original value. If a\n",
    "      certain buffer or fanout breaks the architecture, then that buffer or\n",
    "      fanout is likely the problem.\n",
    "  - If layers can't fit in your number of tiles, you may increase N_TILES\n",
    "    to increase the width of the pipeline. Ensure that the batch size is large\n",
    "    enough to fill the pipeline for each layer, or else one layer can artificially\n",
    "    limit the throughput of the entire DNN.\n",
    "- If the inputs to a DNN layer are signed, ISAAC's input activation energy will\n",
    "  go up significantly (ISAAC will cast the inputs to unsigned numbers, leading\n",
    "  to a large number of 1 bits in inputs), while RAELLA will incur double the\n",
    "  cycles and ADC conversions (from splitting positive/negative inputs into\n",
    "  separate slices). Ensure your models take this into account.\n",
    "\n",
    "## **Glossary**\n",
    "Some vocabulary here differs from the RAELLA paper to better align with the\n",
    "current vocabulary used by the research community. The following is a glossary\n",
    "of terms used in this model:\n",
    "- PIM/CiM: Processing-In-Memory / Compute-In-Memory. Used interchangeably here.\n",
    "- Tile: An architectural tile comprising multiple PIM macros plus buffers,\n",
    "  networks, and quantization units.\n",
    "- Macro: Minimal architecture required to perform operations. In the RAELLA and\n",
    "  ISAAC papers, this is an IMA.\n",
    "- Crossbar: A grid of horizontally/vertically connected memory cells that\n",
    "  perform in-memory operations.\n",
    "- CiM unit: The smallest architectural unit available for mapping. In RAELLA,\n",
    "  this is a 2T2R device. In ISAAC, this is a ReRAM device.\n",
    "- Memory Cell: The smallest unit of memory. In RAELLA, this is a ReRAM device.\n",
    "  In ISAAC, this is a ReRAM (1R) device. Note that a CiM unit may be comprised\n",
    "  of multiple memory cells.\n",
    "\n",
    "The following is an explanation of each dimension that may appear in the\n",
    "problem, mapping, architecture, and stats files. The problem dimensions refer\n",
    "to dimensions of the input, output, and weight tensors that are used in each\n",
    "DNN layer:\n",
    "- P: The width of the output tensor.\n",
    "- Q: The height of the output tensor.\n",
    "- R: The width of the weight filter. 1 for fully-connected layers.\n",
    "- S: The height of the weight filter. 1 for fully-connected layers.\n",
    "- C: The number of input channels.\n",
    "- M: The number of output channels.\n",
    "- G: The number of groups. 1 for non-grouped layers.\n",
    "- N: The batch size.\n",
    "- I: The number of input bits (resolution of inputs).\n",
    "- L: The number of weight bits (resolution of weights).\n",
    "- T: The number of output bits (resolution of outputs. Note that psum\n",
    "  resolution is up to 2*T before quantization).\n",
    "\n",
    "## **Contact**\n",
    "If you have any questions, comments, or concerns, please contact Tanner\n",
    "Andrulis at andrulis@mit.edu. I'm happy to help with anything related to\n",
    "RAELLA, Accelergy, or PIM modeling in Timeloop.\n",
    "\n",
    "## **References**\n",
    "1. Tanner Andrulis, Joel S. Emer, and Vivienne Sze. 2023. RAELLA: Reforming the\n",
    "Arithmetic for Efficient, Low-Resolution, and Low-Loss Analog PIM: No\n",
    "Retraining Required! In Proceedings of the 50th Annual International Symposium\n",
    "on Computer Architecture (ISCA '23). Association for Computing Machinery, New\n",
    "York, NY, USA, Article 27, 1–16. https://doi.org/10.1145/3579371.3589062\n",
    "2. Ali Shafiee, Anirban Nag, Naveen Muralimanohar, \n",
    "Rajeev Balasubramonian\n",
    " John Paul Strachan,\n",
    "Miao Hu, R. Stanley Williams, and Vivek Srikumar, \"ISAA\n",
    ": A Convolutional Neural Network Accelerator wi h\n",
    "In-Situ Analog Arithmetic \n",
    "n Crossbars,\" 2016 ACM/IEEE 43rd Annu l\n",
    "International Symposium on Comput\n",
    "r Architecture (ISCA), Seoul, Korea (South ,\n",
    "2016, pp. 14-26, doi: 10.1109/ISCA.2016.124\n",
    "3. Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew\n",
    "Wojna. 2016. Rethinking the Inception Architecture for Computer Vision. In 2016\n",
    "IEEE Conference on Computer Vision and Pattern Recognition (CVPR). 2818–2826.\n",
    "https://doi.org/10.1109/CVPR.2016.305\n",
    "4. Christian Szegedy, Wei Liu, Yangqing Jia, Pierre Sermanet, Scott Reed,\n",
    "Dragomir Anguelov, Dumitru Erhan, Vincent Vanhoucke, and Andrew Rabinovich.\n",
    "2015. Going deeper with convolutions. In 2015 IEEE Conference on Computer\n",
    "Vision and Pattern Recognition (CVPR). 1–9.\n",
    "https://doi.org/10.1109/CVPR.2015.7298594\n",
    "5. Kaiming He, X. Zhang, Shaoqing Ren, and Jian Sun. 2016. Deep Residual\n",
    "Learning for Image Recognition. 2016 IEEE Conference on Computer Vision and\n",
    "Pattern Recognition (CVPR) (2016), 770–778.\n",
    "6. Mark Sandler, Andrew Howard, Menglong Zhu, Andrey Zhmoginov, and LiangChieh\n",
    "Chen. 2018. MobileNetV2: Inverted Residuals and Linear Bottlenecks. 4510–4520.\n",
    "https://doi.org/10.1109/CVPR.2018.00474\n",
    "7. Ningning Ma, Xiangyu Zhang, Hai-Tao Zheng, and Jian Sun. 2018. ShuffleNet\n",
    "V2: Practical Guidelines for Efficient CNN Architecture Design. In Proceedings\n",
    "of the European Conference on Computer Vision (ECCV).\n",
    "8. Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones,\n",
    "Aidan N Gomez, Ł ukasz Kaiser, and Illia Polosukhin. 2017. Attention is All you\n",
    "Need. In Advances in Neural Information Processing Systems, I. Guyon, U. Von\n",
    "Luxburg, S. Bengio, H. Wallach, R. Fergus, S. Vishwanathan, and R. Garnett\n",
    "(Eds.), Vol. 30. Curran Associates, Inc.\n",
    "https://proceedings.neurips.cc/paper/2017/file/3f5ee243547dee91fbd053c1c4a845aa-Paper.pdf\n",
    "9. Wu, H., Judd, P., Zhang, X., Isaev, M. and Micikevicius, P., 2020. Integer\n",
    "quantization for deep learning inference: Principles and empirical evaluation.\n",
    "arXiv preprint arXiv:2004.09602. https://arxiv.org/abs/2004.09602\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
